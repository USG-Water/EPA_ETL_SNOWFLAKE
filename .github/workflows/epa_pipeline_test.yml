name: EPA Pipeline Test

on:
  # Run on every push to any branch
  push:
    branches:
      - '**'
    paths:
      - '**.py'
      - 'requirements.txt'
      - '.github/workflows/epa_pipeline_test.yml'
  
  # Run on pull requests
  pull_request:
    branches:
      - main
      - master
  
  # Allow manual trigger for debugging
  workflow_dispatch:
    inputs:
      row_limit:
        description: 'Number of rows to test per file'
        required: false
        default: '1000'
        type: string

env:
  PYTHON_VERSION: '3.9'
  TEST_ROW_LIMIT: ${{ github.event.inputs.row_limit || '1000' }}
  
jobs:
  test-pipeline:
    name: Test EPA Data Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Much shorter timeout for tests
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache Python Dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.local
            /opt/hostedtoolcache/Python/*/x64/lib/python*/site-packages
          key: ${{ runner.os }}-pip-test-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-test-
            ${{ runner.os }}-pip-
      
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run Pipeline Tests
        env:
          # Use test schema instead of production
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_ROLE: ${{ secrets.SNOWFLAKE_ROLE }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: EPA_TEST_RESULTS  # Test schema
          SNOWFLAKE_PRIVATE_KEY: ${{ secrets.SNOWFLAKE_PRIVATE_KEY }}
          TEST_MODE: "true"
          MAX_ROWS_PER_TABLE: ${{ env.TEST_ROW_LIMIT }}
        run: |
          echo "=== EPA Pipeline Test Run ==="
          echo "Timestamp: $(date)"
          echo "Commit: ${{ github.sha }}"
          echo "Branch: ${{ github.ref }}"
          echo "Test row limit: ${MAX_ROWS_PER_TABLE}"
          
          # Create the test runner script
          cat > test_pipeline.py << 'EOF'
          import os
          import sys
          import logging
          from datetime import datetime
          
          # Set up logging
          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger(__name__)
          
          # Set test mode environment variables
          os.environ['TEST_MODE'] = 'true'
          max_rows = os.environ.get('MAX_ROWS_PER_TABLE', '1000')
          
          logger.info(f"Starting pipeline test with {max_rows} rows per table")
          
          # Track results
          results = {
              'sdwa': {'status': 'pending', 'tables': 0, 'rows': 0, 'errors': []},
              'frs': {'status': 'pending', 'tables': 0, 'rows': 0, 'errors': []}
          }
          
          # Test SDWA loader
          try:
              logger.info("\n=== Testing SDWA Loader ===")
              
              # Check if file exists
              if os.path.exists('epa_sdwa_loader.py'):
                  script_path = 'epa_sdwa_loader.py'
              elif os.path.exists('scripts/epa_sdwa_loader.py'):
                  script_path = 'scripts/epa_sdwa_loader.py'
              else:
                  raise FileNotFoundError("Cannot find epa_sdwa_loader.py")
              
              # Import and modify for testing
              import importlib.util
              spec = importlib.util.spec_from_file_location("epa_sdwa_loader", script_path)
              sdwa_module = importlib.util.module_from_spec(spec)
              
              # Monkey-patch for testing
              original_load = sdwa_module.SnowflakeKeyPairLoader.load_csv_to_table if hasattr(sdwa_module, 'SnowflakeKeyPairLoader') else None
              
              def test_load_csv_to_table(self, csv_path, table_name, chunk_size=1000, load_id=None):
                  """Modified load function that only loads first N rows for testing"""
                  import pandas as pd
                  
                  # Only load first N rows for testing
                  max_rows = int(os.environ.get('MAX_ROWS_PER_TABLE', '1000'))
                  logger.info(f"TEST MODE: Loading max {max_rows} rows for {table_name}")
                  
                  # Read limited rows
                  df = pd.read_csv(csv_path, nrows=max_rows, dtype=str, na_filter=False)
                  
                  # Add TEST_ prefix to table name
                  table_name = f"TEST_{table_name}"
                  
                  # Call original method with limited data
                  temp_csv = f"/tmp/test_{table_name}.csv"
                  df.to_csv(temp_csv, index=False)
                  
                  # Use smaller chunk size for test
                  result = original_load(self, temp_csv, table_name, chunk_size=500, load_id=load_id)
                  
                  # Clean up temp file
                  if os.path.exists(temp_csv):
                      os.remove(temp_csv)
                  
                  return min(result, max_rows) if result else 0
              
              if original_load:
                  sdwa_module.SnowflakeKeyPairLoader.load_csv_to_table = test_load_csv_to_table
              
              # Run the SDWA loader
              spec.loader.exec_module(sdwa_module)
              
              # Call main function
              exit_code = sdwa_module.main()
              
              if exit_code == 0:
                  results['sdwa']['status'] = 'success'
                  logger.info("✅ SDWA loader test passed")
              else:
                  results['sdwa']['status'] = 'failed'
                  results['sdwa']['errors'].append(f"Exit code: {exit_code}")
                  logger.error(f"❌ SDWA loader test failed with exit code {exit_code}")
                  
          except Exception as e:
              results['sdwa']['status'] = 'error'
              results['sdwa']['errors'].append(str(e))
              logger.error(f"❌ SDWA loader test error: {e}")
          
          # Test FRS loader (if it exists)
          try:
              logger.info("\n=== Testing FRS Loader ===")
              
              # Check if file exists
              if os.path.exists('epa_frs_loader.py'):
                  script_path = 'epa_frs_loader.py'
              elif os.path.exists('scripts/epa_frs_loader.py'):
                  script_path = 'scripts/epa_frs_loader.py'
              else:
                  logger.warning("FRS loader not found, skipping")
                  results['frs']['status'] = 'skipped'
              
              if results['frs']['status'] != 'skipped':
                  spec = importlib.util.spec_from_file_location("epa_frs_loader", script_path)
                  frs_module = importlib.util.module_from_spec(spec)
                  
                  # Apply same monkey-patch for FRS
                  if hasattr(frs_module, 'SnowflakeFRSLoader'):
                      original_frs_load = frs_module.SnowflakeFRSLoader.load_csv_to_table
                      
                      def test_frs_load_csv_to_table(self, csv_path, table_name, chunk_size=1000, load_id=None):
                          import pandas as pd
                          max_rows = int(os.environ.get('MAX_ROWS_PER_TABLE', '1000'))
                          logger.info(f"TEST MODE: Loading max {max_rows} rows for {table_name}")
                          
                          df = pd.read_csv(csv_path, nrows=max_rows, dtype=str, na_filter=False)
                          table_name = f"TEST_{table_name}"
                          
                          temp_csv = f"/tmp/test_{table_name}.csv"
                          df.to_csv(temp_csv, index=False)
                          
                          result = original_frs_load(self, temp_csv, table_name, chunk_size=500, load_id=load_id)
                          
                          if os.path.exists(temp_csv):
                              os.remove(temp_csv)
                          
                          return min(result, max_rows) if result else 0
                      
                      frs_module.SnowflakeFRSLoader.load_csv_to_table = test_frs_load_csv_to_table
                  
                  spec.loader.exec_module(frs_module)
                  exit_code = frs_module.main()
                  
                  if exit_code == 0:
                      results['frs']['status'] = 'success'
                      logger.info("✅ FRS loader test passed")
                  else:
                      results['frs']['status'] = 'failed'
                      results['frs']['errors'].append(f"Exit code: {exit_code}")
                      logger.error(f"❌ FRS loader test failed with exit code {exit_code}")
                      
          except Exception as e:
              results['frs']['status'] = 'error'
              results['frs']['errors'].append(str(e))
              logger.error(f"❌ FRS loader test error: {e}")
          
          # Print summary
          logger.info("\n" + "="*50)
          logger.info("TEST SUMMARY")
          logger.info("="*50)
          
          overall_success = True
          for loader, result in results.items():
              status_icon = "✅" if result['status'] == 'success' else "⚠️" if result['status'] == 'skipped' else "❌"
              logger.info(f"{status_icon} {loader.upper()}: {result['status']}")
              if result['errors']:
                  for error in result['errors']:
                      logger.info(f"    Error: {error}")
              if result['status'] not in ['success', 'skipped']:
                  overall_success = False
          
          # Exit with appropriate code
          sys.exit(0 if overall_success else 1)
          EOF
          
          # Run the test script
          python test_pipeline.py
          
          EXIT_CODE=$?
          
          if [ $EXIT_CODE -eq 0 ]; then
              echo "✅ All pipeline tests passed"
          else
              echo "❌ Pipeline tests failed"
              exit 1
          fi
      
      - name: Clean Up Test Tables
        if: always()
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_ROLE: ${{ secrets.SNOWFLAKE_ROLE }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: EPA_TEST_RESULTS
          SNOWFLAKE_PRIVATE_KEY: ${{ secrets.SNOWFLAKE_PRIVATE_KEY }}
        run: |
          # Optional: Clean up test tables after run
          python -c "
          import os
          import snowflake.connector
          from cryptography.hazmat.backends import default_backend
          from cryptography.hazmat.primitives import serialization
          
          try:
              # Parse private key
              private_key_str = os.environ.get('SNOWFLAKE_PRIVATE_KEY')
              private_key_bytes = private_key_str.encode('utf-8')
              private_key = serialization.load_pem_private_key(
                  private_key_bytes,
                  password=None,
                  backend=default_backend()
              )
              private_key_der = private_key.private_bytes(
                  encoding=serialization.Encoding.DER,
                  format=serialization.PrivateFormat.PKCS8,
                  encryption_algorithm=serialization.NoEncryption()
              )
              
              # Connect
              conn = snowflake.connector.connect(
                  account=os.environ['SNOWFLAKE_ACCOUNT'],
                  user=os.environ['SNOWFLAKE_USER'],
                  private_key=private_key_der,
                  warehouse=os.environ['SNOWFLAKE_WAREHOUSE'],
                  database=os.environ['SNOWFLAKE_DATABASE'],
                  schema=os.environ['SNOWFLAKE_SCHEMA'],
                  role=os.environ['SNOWFLAKE_ROLE']
              )
              
              cursor = conn.cursor()
              
              # List test tables
              cursor.execute(\"\"\"
                  SELECT TABLE_NAME 
                  FROM INFORMATION_SCHEMA.TABLES 
                  WHERE TABLE_SCHEMA = 'EPA_TEST_RESULTS' 
                    AND TABLE_NAME LIKE 'TEST_%'
              \"\"\")
              
              tables = cursor.fetchall()
              print(f'Found {len(tables)} test tables')
              
              # Optional: Drop old test tables (uncomment if desired)
              # for (table_name,) in tables:
              #     cursor.execute(f'DROP TABLE IF EXISTS \"{table_name}\"')
              #     print(f'Dropped {table_name}')
              
              cursor.close()
              conn.close()
              
          except Exception as e:
              print(f'Cleanup warning: {e}')
          "
      
      - name: Post Test Summary
        if: always()
        run: |
          echo "=== Test Pipeline Complete ==="
          echo "Commit: ${{ github.sha }}"
          echo "Author: ${{ github.actor }}"
          echo "Test tables created in EPA_TEST_RESULTS schema with TEST_ prefix"
          echo "Only first ${{ env.TEST_ROW_LIMIT }} rows loaded per table"
