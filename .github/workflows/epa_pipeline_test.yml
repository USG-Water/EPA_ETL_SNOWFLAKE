name: EPA Pipeline Test

on:
  # Always allow manual trigger
  workflow_dispatch:
    inputs:
      row_limit:
        description: 'Rows to test per table (default 1000)'
        required: false
        default: '1000'
      file_limit:
        description: 'Max CSV files to process (default 3)'
        required: false
        default: '3'
  
  # Trigger on push to any branch
  push:
    paths:
      - '**.py'
      - 'requirements.txt'
      - '.github/workflows/**'
  
  # Trigger on PRs
  pull_request:
    branches:
      - main
      - master

env:
  PYTHON_VERSION: '3.9'

jobs:
  test-limited-pipeline:
    name: Test EPA Pipeline (Limited Data)
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Short timeout for tests
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run Limited Test
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_ROLE: ${{ secrets.SNOWFLAKE_ROLE }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_PRIVATE_KEY: ${{ secrets.SNOWFLAKE_PRIVATE_KEY }}
          MAX_ROWS_PER_TABLE: ${{ github.event.inputs.row_limit || '1000' }}
          MAX_FILES_TO_PROCESS: ${{ github.event.inputs.file_limit || '3' }}
        run: |
          echo "=== EPA TEST PIPELINE ==="
          echo "Row limit: ${MAX_ROWS_PER_TABLE}"
          echo "File limit: ${MAX_FILES_TO_PROCESS}"
          echo "Schema: EPA_TEST_RESULTS"
          echo "========================"
          
          # Check if test script exists
          if [ -f "test_epa_pipeline.py" ]; then
              python test_epa_pipeline.py
          else
              # Create the test script inline
              cat > test_epa_pipeline.py << 'ENDSCRIPT'
import os, sys, logging, tempfile, zipfile, shutil
from datetime import datetime
import requests, pandas as pd, snowflake.connector
from snowflake.connector.pandas_tools import write_pandas
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logging.getLogger('snowflake.connector').setLevel(logging.WARNING)

MAX_ROWS = int(os.environ.get('MAX_ROWS_PER_TABLE', '1000'))
MAX_FILES = int(os.environ.get('MAX_FILES_TO_PROCESS', '3'))

logger.info(f"TEST CONFIG: {MAX_ROWS} rows max, {MAX_FILES} files max")

# Download and extract first few CSV files
download_dir = tempfile.mkdtemp(prefix="test_epa_")
logger.info(f"Using temp dir: {download_dir}")

try:
    # Download SDWA
    url = "https://echo.epa.gov/files/echodownloads/SDWA_latest_downloads.zip"
    logger.info(f"Downloading from {url}")
    response = requests.get(url, stream=True, timeout=60)
    zip_path = os.path.join(download_dir, "sdwa.zip")
    with open(zip_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    
    # Extract CSVs
    csv_files = []
    with zipfile.ZipFile(zip_path, 'r') as z:
        for name in z.namelist()[:MAX_FILES]:  # Limit files
            if name.endswith('.csv'):
                z.extract(name, download_dir)
                csv_files.append(os.path.join(download_dir, name))
    
    logger.info(f"Processing {len(csv_files)} CSV files")
    
    # Connect to Snowflake
    private_key_str = os.environ['SNOWFLAKE_PRIVATE_KEY']
    private_key_bytes = private_key_str.encode('utf-8')
    private_key = serialization.load_pem_private_key(private_key_bytes, password=None, backend=default_backend())
    private_key_der = private_key.private_bytes(
        encoding=serialization.Encoding.DER,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption()
    )
    
    conn = snowflake.connector.connect(
        account=os.environ['SNOWFLAKE_ACCOUNT'],
        user=os.environ['SNOWFLAKE_USER'],
        private_key=private_key_der,
        warehouse=os.environ['SNOWFLAKE_WAREHOUSE'],
        database=os.environ['SNOWFLAKE_DATABASE'],
        schema='EPA_TEST_RESULTS',
        role=os.environ['SNOWFLAKE_ROLE']
    )
    
    logger.info("Connected to Snowflake EPA_TEST_RESULTS schema")
    
    # Process each CSV with row limit
    for csv_file in csv_files:
        filename = os.path.basename(csv_file).replace('.csv', '')
        table_name = f"TEST_{filename}".upper()
        
        logger.info(f"Loading {MAX_ROWS} rows to {table_name}")
        
        # Read LIMITED rows only
        df = pd.read_csv(csv_file, nrows=MAX_ROWS, dtype=str, na_filter=False)
        df.columns = [c.replace(' ', '_').replace('-', '_').upper() for c in df.columns]
        df['LOAD_TIMESTAMP'] = datetime.now()
        df['TEST_RUN'] = True
        df['ROW_COUNT'] = len(df)
        
        # Create table
        cols = [f'"{c}" STRING' for c in df.columns]
        ddl = f"CREATE OR REPLACE TABLE {table_name} ({', '.join(cols)})"
        conn.cursor().execute(ddl)
        
        # Load data
        success, _, _, _ = write_pandas(conn, df, table_name, auto_create_table=False, quote_identifiers=True)
        
        if success:
            logger.info(f"✅ Loaded {len(df)} rows to {table_name}")
        else:
            logger.error(f"❌ Failed to load {table_name}")
    
    conn.close()
    logger.info("Test complete!")
    
finally:
    shutil.rmtree(download_dir, ignore_errors=True)
ENDSCRIPT
              
              python test_epa_pipeline.py
          fi
      
      - name: Summary
        if: always()
        run: |
          echo "=== TEST COMPLETE ==="
          echo "Tables created in EPA_TEST_RESULTS schema"
          echo "All tables prefixed with TEST_"
          echo "Limited to ${{ github.event.inputs.row_limit || '1000' }} rows per table"
          echo "Limited to ${{ github.event.inputs.file_limit || '3' }} files"
